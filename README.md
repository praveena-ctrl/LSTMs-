# LSTMs-
This project presents a high-performance LSTM-based deep learning model designed to forecast future temperature trends using the Jena Climate dataset. By leveraging the strength of Long Short-Term Memory networks, the model captures long-range temporal dependencies, learns complex climate patterns, and delivers highly accurate next-step prediction.
Attention-Based LSTM for Time Series PredictionThis project implements an Attention-based Long Short-Term Memory (LSTM) neural network using TensorFlow/Keras to predict the next step of a generated synthetic sine wave time series. The inclusion of an attention mechanism allows the model to dynamically weigh the importance of different time steps in the input sequence when making a prediction.

Getting StartedThese instructions will get you a copy of the project up and running on your local machine or a cloud environment like Google Colab.PrerequisitesThe script requires the following Python libraries:Bashpip install tensorflow numpy pandas matplotlib scikit-learn
Running the ScriptThe provided Python script (lstm.py) contains all steps from data generation to model evaluation.Save the file: Save the provided code as lstm.py.Execute: Run the script from your terminal:Bashpython lstm.py
Alternatively, if running in a Jupyter/Colab environment, all cells will execute sequentially.

Project Structure & OutputsThe script automatically creates an outputs/ directory and populates it with training artifacts, model files, and visualizations.PathDescriptionoutputs/Root directory for all generated files.outputs/logs/training_logs.jsonTraining history (loss, validation loss) for all epochs.outputs/models/best_model.h5The best model saved during training (based on minimum validation loss).outputs/models/final_model.h5The final model after all training epochs.outputs/predictions/dataset_preview.csvA preview of the generated target values ($y$ data).outputs/predictions/val_predictions.csvA CSV file comparing the actual and predicted values on the validation set.outputs/predictions/attention_weights.npyNumpy array containing the attention weights for all validation samples.outputs/plots/Directory for all generated visualizations.

Model ArchitectureThe core of the project is a sequence-to-single-value prediction model enhanced with a custom Attention Layer.

Custom LayersExpandDimsLayer: Prepares the 1D input sequence for the 2D-expecting LSTM layer.AttentionLayer:Takes the sequence of hidden states from the LSTM (return_sequences=True).Computes a score for each time step using a Dense(1) layer.Applies a softmax activation to the scores to get normalized attention weights.Calculates the context vector as the weighted sum of the hidden states.Keras Model SummaryLayer (type)Output ShapeParam #input_1 (InputLayer)[(None, 50)]0expand_dims_layer (ExpandDimsLayer)(None, 50, 1)0lstm (LSTM)(None, 50, 64)16896attention_layer (AttentionLayer)[(None, 64), (None, 50, 1)]65dense_1 (Dense)(None, 1)65Total params: 17,026Trainable params: 17,026 Results and PerformanceTraining and MetricsThe model was trained for up to 50 epochs with a batch size of 32, using Adam optimization and Mean Squared Error (MSE) loss. Early stopping and model checkpointing were used to prevent overfitting and save the best performing model based on val_loss.MetricValueMean Squared Error (MSE)$0.0039$Mean Absolute Error (MAE)$0.0505$R-squared (R2 Score)$0.9840$Correlation (Actual vs. Predicted)$0.9920$The high $R^2$ score and correlation indicate excellent fit and predictive power on the validation data.VisualizationsThe script generates several plots in outputs/plots/:training_loss_curve.png: Visualizes the training and validation loss over epochs.forecast_plot.png / val_predictions_visualization.png: Compares the model's predictions against the actual values for the validation set, demonstrating its accuracy.attention_sample0.png: A stem plot of the attention weights for the first sample in the validation set, showing which parts of the input sequence the model focused on.A preview of the validation performance:Would you like to explore the attention weights for a specific sample index, or would you like to see the loss curve visualization?
